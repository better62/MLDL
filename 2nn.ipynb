{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEoabX-hGCh"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfcNKryaj6VJ",
        "outputId": "ac404cbc-7d96-4bfd-b212-e38c40fb131f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6CbjnM5kbBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8dddef-a9cd-4ca4-86f4-85de7ad1785e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PATH/TO/MNIST_DATA_FILE\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/PATH/TO/MNIST_DATA_FILE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyammZP8hI7P"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from mnist.data_utils import load_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxTNOvI5NHD"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuQB6W2U5ZE2"
      },
      "outputs": [],
      "source": [
        "from operator import xor\n",
        "def elu(z, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Implement the elu activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    Please DO NOT MODIFY the alpha value.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "    z[z<0] = alpha*(np.exp(z[z<0])-1)\n",
        "    return z\n",
        "    #####################\n",
        "\n",
        "\n",
        "def softmax(X):\n",
        "    \"\"\"\n",
        "    Implement the softmax function.\n",
        "    The method takes the input X and returns the output of the function.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "    # https://gist.github.com/claudiodsf/6596204\n",
        "    exp_X = np.exp(X[:,])\n",
        "    X = exp_X/np.sum(exp_X, axis=1)[:, np.newaxis]\n",
        "    return X\n",
        "    #####################\n",
        "\n",
        "def deriv_elu(x, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Implement the derivative of elu activation function.\n",
        "    The method takes the input z and returns the output of the function.\n",
        "    Please DO NOT MODIFY the alpha value.\n",
        "\n",
        "    Question (a)\n",
        "\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "    x[x>=0] = 1\n",
        "    x[x<0] = alpha*np.exp(x[x<0])\n",
        "    x = np.sum(x, axis=0)/x.shape[1]\n",
        "    result = np.identity(x.shape[0])*x\n",
        "    return result\n",
        "    #####################\n",
        "\n",
        "def load_batch(X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches with the remainder dropped.\n",
        "\n",
        "    Do NOT modify this function\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        permutation = np.random.permutation(X.shape[0])\n",
        "        X = X[permutation, :]\n",
        "        Y = Y[permutation, :]\n",
        "    num_steps = int(X.shape[0])//batch_size\n",
        "    step = 0\n",
        "    while step<num_steps:\n",
        "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
        "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
        "        step+=1\n",
        "        yield X_batch, Y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsU8v_6khR30"
      },
      "source": [
        "# 2-Layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA5udiGmhRb5"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNN:\n",
        "    \"\"\" a neural network with 2 layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        Do NOT modify this function.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n",
        "\n",
        "    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n",
        "        \"\"\"\n",
        "        initializes parameters with Xavier Initialization.\n",
        "\n",
        "        Question (b)\n",
        "        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n",
        "\n",
        "        Inputs\n",
        "        - input_dim\n",
        "        - num_hiddens\n",
        "        - num_classes\n",
        "        Returns\n",
        "        - params: a dictionary with the initialized parameters.\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        ##### YOUR CODE #####\n",
        "        params['input_dim'] = input_dim\n",
        "        params['num_hiddens'] = num_hiddens\n",
        "        params['num_classes'] = num_classes\n",
        "        params['W1'] = np.random.uniform(-np.sqrt(6)/np.sqrt(input_dim+num_hiddens), np.sqrt(6)/np.sqrt(input_dim+num_hiddens), (input_dim, num_hiddens))\n",
        "        params['b1'] = np.zeros((num_hiddens,))\n",
        "        params['W2'] = np.random.uniform(-np.sqrt(6)/np.sqrt(num_hiddens+num_classes), np.sqrt(6)/np.sqrt(num_hiddens+num_classes), (num_hiddens, num_classes))\n",
        "        params['b2'] = np.zeros((num_classes,))\n",
        "        #####################\n",
        "        return params\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Define and perform the feed forward step of a two-layer neural network.\n",
        "        Specifically, the network structue is given by\n",
        "\n",
        "          y = softmax(ELU(X W1 + b1) W2 + b2)\n",
        "\n",
        "        where X is the input matrix of shape (N, D), y is the class distribution matrix\n",
        "        of shape (N, C), N is the number of examples (either the entire dataset or\n",
        "        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n",
        "\n",
        "        Question (c)\n",
        "        - ff_dict will be used to run backpropagation in backward method.\n",
        "\n",
        "        Inputs\n",
        "        - X: the input matrix of shape (N, D)\n",
        "\n",
        "        Returns\n",
        "        - y: the output of the model\n",
        "        - ff_dict: a dictionary with all the fully connected units and activations.\n",
        "        \"\"\"\n",
        "        ff_dict = {}\n",
        "        ##### YOUR CODE #####\n",
        "        ff_dict['XW1'] = X.dot(self.params['W1'])\n",
        "        ff_dict['+b1'] = ff_dict['XW1'] + self.params['b1']\n",
        "        ff_dict['elu'] = elu(ff_dict['+b1'])\n",
        "        ff_dict['*W2'] = ff_dict['elu'].dot(self.params['W2'])\n",
        "        ff_dict['+b2'] = ff_dict['*W2']+self.params['b2']\n",
        "        ff_dict['softmax'] = softmax(ff_dict['+b2'])\n",
        "        y = ff_dict['softmax']\n",
        "        #####################\n",
        "        return y, ff_dict\n",
        "\n",
        "    def backward(self, X, Y, ff_dict):\n",
        "        \"\"\"\n",
        "        Performs backpropagation over the two-layer neural network, and returns\n",
        "        a dictionary of gradients of all model parameters.\n",
        "\n",
        "        Question (d)\n",
        "\n",
        "        Inputs:\n",
        "         - X: the input matrix of shape (B, D), where B is the number of examples\n",
        "              in a mini-batch, D is the feature dimensionality.\n",
        "         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n",
        "              where B is the number of examples in a mini-batch, C is the number\n",
        "              of classes.\n",
        "         - ff_dict: the dictionary containing all the fully connected units and\n",
        "              activations.\n",
        "\n",
        "        Returns:\n",
        "         - grads: a dictionary containing the gradients of corresponding weights and biases.\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        ##### YOUR CODE #####\n",
        "        dl = ff_dict['softmax']-Y\n",
        "        grads['db2'] = np.sum(dl, axis=0)\n",
        "        grads['dW2'] = ff_dict['elu'].T.dot(dl)\n",
        "        dh = dl.dot(self.params['W2'].T).dot(deriv_elu(ff_dict['+b1']))\n",
        "        grads['db1'] = np.sum(dh, axis=0)\n",
        "        grads['dW1'] = X.T.dot(dh)\n",
        "        #####################\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes cross entropy loss.\n",
        "\n",
        "        Do NOT modify this function.\n",
        "\n",
        "        Inputs\n",
        "            Y:\n",
        "            Y_hat:\n",
        "        Returns\n",
        "            loss:\n",
        "        \"\"\"\n",
        "        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n",
        "        \"\"\"\n",
        "        Runs mini-batch gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X\n",
        "        - Y\n",
        "        - X_val\n",
        "        - Y_Val\n",
        "        - lr\n",
        "        - n_epochs\n",
        "        - batch_size\n",
        "        - log_interval\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "          for X_batch, Y_batch in load_batch(X, Y, batch_size):\n",
        "              self.train_step(X_batch, Y_batch, batch_size, lr)\n",
        "          if epoch % log_interval==0:\n",
        "              Y_hat, ff_dict = self.forward(X)\n",
        "              train_loss = self.compute_loss(Y, Y_hat)\n",
        "              train_acc = self.evaluate(Y, Y_hat)\n",
        "              Y_hat, ff_dict = self.forward(X_val)\n",
        "              valid_loss = self.compute_loss(Y_val, Y_hat)\n",
        "              valid_acc = self.evaluate(Y_val, Y_hat)\n",
        "              print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n",
        "                    format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch, batch_size, lr):\n",
        "        \"\"\"\n",
        "        Updates the parameters using gradient descent.\n",
        "\n",
        "        Do NOT Modify this method.\n",
        "\n",
        "        Inputs\n",
        "        - X_batch\n",
        "        - Y_batch\n",
        "        - batch_size\n",
        "        - lr\n",
        "        \"\"\"\n",
        "        _, ff_dict = self.forward(X_batch)\n",
        "        grads = self.backward(X_batch, Y_batch, ff_dict)\n",
        "        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n",
        "        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n",
        "        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n",
        "        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n",
        "\n",
        "    def evaluate(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Computes classification accuracy.\n",
        "\n",
        "        Do NOT modify this function\n",
        "\n",
        "        Inputs\n",
        "        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n",
        "             where C is the number of classes.\n",
        "        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n",
        "             where C is the number of classes.\n",
        "\n",
        "        Returns\n",
        "            accuracy: the classification accuracy in float\n",
        "        \"\"\"\n",
        "        classes_pred = np.argmax(Y_hat, axis=1)\n",
        "        classes_gt = np.argmax(Y, axis=1)\n",
        "        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM2lWhtDYC6"
      },
      "source": [
        "# Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48ooR6YIxYhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e95020-646d-421a-ae50-137dc04f5197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded:\n",
            "Training data shape: (60000, 784)\n",
            "Training labels shape: (60000, 10)\n",
            "Test data shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "\n",
            "Set validation data aside\n",
            "Training data shape:  (48000, 784)\n",
            "Training labels shape:  (48000, 10)\n",
            "Validation data shape:  (12000, 784)\n",
            "Validation labels shape:  (12000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train, X_test, Y_test = load_data()\n",
        "\n",
        "idxs = np.arange(len(X_train))\n",
        "np.random.shuffle(idxs)\n",
        "split_idx = int(np.ceil(len(idxs)*0.8))\n",
        "X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n",
        "X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n",
        "print()\n",
        "print('Set validation data aside')\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', Y_train.shape)\n",
        "print('Validation data shape: ', X_valid.shape)\n",
        "print('Validation labels shape: ', Y_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzw-D4Zr5xoi"
      },
      "source": [
        "# Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlnC_rerHPaN"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Question (e)\n",
        "# Tune the hyperparameters with validation data,\n",
        "# and print the results by running the lines below.\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCqVT4S0Tm5"
      },
      "outputs": [],
      "source": [
        "# model instantiation\n",
        "model = TwoLayerNN(input_dim=784, num_hiddens=100, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cWb6xg0NxOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931e133f-5bfe-43d2-815b-ab172ac4db07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 00 - train loss/acc: 0.318 0.910, valid loss/acc: 0.334 0.902\n",
            "epoch 01 - train loss/acc: 0.285 0.920, valid loss/acc: 0.307 0.913\n",
            "epoch 02 - train loss/acc: 0.273 0.923, valid loss/acc: 0.301 0.913\n",
            "epoch 03 - train loss/acc: 0.258 0.928, valid loss/acc: 0.288 0.918\n",
            "epoch 04 - train loss/acc: 0.256 0.928, valid loss/acc: 0.286 0.917\n",
            "epoch 05 - train loss/acc: 0.244 0.931, valid loss/acc: 0.277 0.920\n",
            "epoch 06 - train loss/acc: 0.245 0.930, valid loss/acc: 0.279 0.917\n",
            "epoch 07 - train loss/acc: 0.234 0.932, valid loss/acc: 0.269 0.921\n",
            "epoch 08 - train loss/acc: 0.233 0.933, valid loss/acc: 0.272 0.920\n",
            "epoch 09 - train loss/acc: 0.225 0.935, valid loss/acc: 0.266 0.922\n",
            "epoch 10 - train loss/acc: 0.222 0.937, valid loss/acc: 0.264 0.923\n",
            "epoch 11 - train loss/acc: 0.221 0.937, valid loss/acc: 0.264 0.923\n",
            "epoch 12 - train loss/acc: 0.213 0.939, valid loss/acc: 0.259 0.924\n",
            "epoch 13 - train loss/acc: 0.210 0.940, valid loss/acc: 0.253 0.926\n",
            "epoch 14 - train loss/acc: 0.213 0.940, valid loss/acc: 0.261 0.924\n",
            "epoch 15 - train loss/acc: 0.206 0.942, valid loss/acc: 0.253 0.927\n",
            "epoch 16 - train loss/acc: 0.202 0.943, valid loss/acc: 0.251 0.926\n",
            "epoch 17 - train loss/acc: 0.205 0.943, valid loss/acc: 0.255 0.924\n",
            "epoch 18 - train loss/acc: 0.201 0.942, valid loss/acc: 0.251 0.925\n",
            "epoch 19 - train loss/acc: 0.197 0.943, valid loss/acc: 0.247 0.927\n",
            "epoch 20 - train loss/acc: 0.196 0.944, valid loss/acc: 0.249 0.924\n",
            "epoch 21 - train loss/acc: 0.195 0.943, valid loss/acc: 0.248 0.928\n",
            "epoch 22 - train loss/acc: 0.216 0.936, valid loss/acc: 0.268 0.919\n",
            "epoch 23 - train loss/acc: 0.188 0.947, valid loss/acc: 0.243 0.928\n",
            "epoch 24 - train loss/acc: 0.187 0.947, valid loss/acc: 0.243 0.928\n",
            "epoch 25 - train loss/acc: 0.183 0.948, valid loss/acc: 0.240 0.929\n",
            "epoch 26 - train loss/acc: 0.184 0.947, valid loss/acc: 0.242 0.931\n",
            "epoch 27 - train loss/acc: 0.189 0.945, valid loss/acc: 0.245 0.927\n",
            "epoch 28 - train loss/acc: 0.183 0.949, valid loss/acc: 0.243 0.929\n",
            "epoch 29 - train loss/acc: 0.179 0.950, valid loss/acc: 0.238 0.931\n",
            "epoch 30 - train loss/acc: 0.184 0.947, valid loss/acc: 0.243 0.929\n",
            "epoch 31 - train loss/acc: 0.183 0.947, valid loss/acc: 0.243 0.929\n",
            "epoch 32 - train loss/acc: 0.174 0.951, valid loss/acc: 0.234 0.932\n",
            "epoch 33 - train loss/acc: 0.173 0.951, valid loss/acc: 0.234 0.931\n",
            "epoch 34 - train loss/acc: 0.176 0.949, valid loss/acc: 0.236 0.932\n",
            "epoch 35 - train loss/acc: 0.177 0.949, valid loss/acc: 0.240 0.928\n",
            "epoch 36 - train loss/acc: 0.167 0.953, valid loss/acc: 0.229 0.933\n",
            "epoch 37 - train loss/acc: 0.170 0.952, valid loss/acc: 0.233 0.932\n",
            "epoch 38 - train loss/acc: 0.187 0.943, valid loss/acc: 0.247 0.926\n",
            "epoch 39 - train loss/acc: 0.175 0.949, valid loss/acc: 0.239 0.929\n",
            "epoch 40 - train loss/acc: 0.163 0.954, valid loss/acc: 0.228 0.932\n",
            "epoch 41 - train loss/acc: 0.159 0.955, valid loss/acc: 0.224 0.932\n",
            "epoch 42 - train loss/acc: 0.163 0.954, valid loss/acc: 0.228 0.932\n",
            "epoch 43 - train loss/acc: 0.163 0.953, valid loss/acc: 0.232 0.932\n",
            "epoch 44 - train loss/acc: 0.162 0.954, valid loss/acc: 0.227 0.933\n",
            "epoch 45 - train loss/acc: 0.162 0.953, valid loss/acc: 0.229 0.933\n",
            "epoch 46 - train loss/acc: 0.164 0.953, valid loss/acc: 0.230 0.934\n",
            "epoch 47 - train loss/acc: 0.175 0.948, valid loss/acc: 0.244 0.928\n",
            "epoch 48 - train loss/acc: 0.160 0.954, valid loss/acc: 0.226 0.933\n",
            "epoch 49 - train loss/acc: 0.161 0.952, valid loss/acc: 0.229 0.932\n",
            "epoch 50 - train loss/acc: 0.155 0.956, valid loss/acc: 0.221 0.935\n",
            "epoch 51 - train loss/acc: 0.163 0.950, valid loss/acc: 0.234 0.929\n",
            "epoch 52 - train loss/acc: 0.149 0.957, valid loss/acc: 0.218 0.936\n",
            "epoch 53 - train loss/acc: 0.158 0.954, valid loss/acc: 0.229 0.931\n",
            "epoch 54 - train loss/acc: 0.151 0.957, valid loss/acc: 0.221 0.934\n",
            "epoch 55 - train loss/acc: 0.147 0.958, valid loss/acc: 0.216 0.936\n",
            "epoch 56 - train loss/acc: 0.155 0.955, valid loss/acc: 0.223 0.933\n",
            "epoch 57 - train loss/acc: 0.147 0.957, valid loss/acc: 0.218 0.937\n",
            "epoch 58 - train loss/acc: 0.147 0.957, valid loss/acc: 0.216 0.935\n",
            "epoch 59 - train loss/acc: 0.144 0.959, valid loss/acc: 0.215 0.937\n",
            "epoch 60 - train loss/acc: 0.142 0.959, valid loss/acc: 0.214 0.937\n",
            "epoch 61 - train loss/acc: 0.148 0.957, valid loss/acc: 0.223 0.933\n",
            "epoch 62 - train loss/acc: 0.148 0.958, valid loss/acc: 0.221 0.935\n",
            "epoch 63 - train loss/acc: 0.141 0.959, valid loss/acc: 0.216 0.939\n",
            "epoch 64 - train loss/acc: 0.142 0.958, valid loss/acc: 0.215 0.936\n",
            "epoch 65 - train loss/acc: 0.150 0.956, valid loss/acc: 0.223 0.934\n",
            "epoch 66 - train loss/acc: 0.143 0.958, valid loss/acc: 0.219 0.936\n",
            "epoch 67 - train loss/acc: 0.143 0.959, valid loss/acc: 0.217 0.936\n",
            "epoch 68 - train loss/acc: 0.140 0.959, valid loss/acc: 0.213 0.937\n",
            "epoch 69 - train loss/acc: 0.140 0.958, valid loss/acc: 0.215 0.938\n",
            "epoch 70 - train loss/acc: 0.161 0.948, valid loss/acc: 0.239 0.929\n",
            "epoch 71 - train loss/acc: 0.138 0.959, valid loss/acc: 0.211 0.939\n",
            "epoch 72 - train loss/acc: 0.143 0.958, valid loss/acc: 0.219 0.936\n",
            "epoch 73 - train loss/acc: 0.143 0.957, valid loss/acc: 0.217 0.936\n",
            "epoch 74 - train loss/acc: 0.133 0.961, valid loss/acc: 0.209 0.939\n",
            "epoch 75 - train loss/acc: 0.138 0.960, valid loss/acc: 0.215 0.937\n",
            "epoch 76 - train loss/acc: 0.133 0.961, valid loss/acc: 0.208 0.941\n",
            "epoch 77 - train loss/acc: 0.135 0.960, valid loss/acc: 0.213 0.938\n",
            "epoch 78 - train loss/acc: 0.137 0.959, valid loss/acc: 0.218 0.936\n",
            "epoch 79 - train loss/acc: 0.139 0.959, valid loss/acc: 0.214 0.938\n",
            "epoch 80 - train loss/acc: 0.130 0.962, valid loss/acc: 0.208 0.939\n",
            "epoch 81 - train loss/acc: 0.153 0.952, valid loss/acc: 0.231 0.932\n",
            "epoch 82 - train loss/acc: 0.145 0.957, valid loss/acc: 0.220 0.934\n",
            "epoch 83 - train loss/acc: 0.126 0.963, valid loss/acc: 0.204 0.941\n",
            "epoch 84 - train loss/acc: 0.133 0.960, valid loss/acc: 0.213 0.939\n",
            "epoch 85 - train loss/acc: 0.143 0.956, valid loss/acc: 0.224 0.934\n",
            "epoch 86 - train loss/acc: 0.127 0.962, valid loss/acc: 0.206 0.940\n",
            "epoch 87 - train loss/acc: 0.125 0.963, valid loss/acc: 0.205 0.940\n",
            "epoch 88 - train loss/acc: 0.126 0.963, valid loss/acc: 0.203 0.940\n",
            "epoch 89 - train loss/acc: 0.138 0.958, valid loss/acc: 0.218 0.937\n",
            "epoch 90 - train loss/acc: 0.133 0.960, valid loss/acc: 0.212 0.937\n",
            "epoch 91 - train loss/acc: 0.129 0.962, valid loss/acc: 0.210 0.939\n",
            "epoch 92 - train loss/acc: 0.133 0.961, valid loss/acc: 0.210 0.939\n",
            "epoch 93 - train loss/acc: 0.124 0.963, valid loss/acc: 0.205 0.939\n",
            "epoch 94 - train loss/acc: 0.124 0.963, valid loss/acc: 0.202 0.941\n",
            "epoch 95 - train loss/acc: 0.127 0.962, valid loss/acc: 0.208 0.938\n",
            "epoch 96 - train loss/acc: 0.125 0.963, valid loss/acc: 0.206 0.940\n",
            "epoch 97 - train loss/acc: 0.118 0.965, valid loss/acc: 0.200 0.942\n",
            "epoch 98 - train loss/acc: 0.121 0.963, valid loss/acc: 0.202 0.941\n",
            "epoch 99 - train loss/acc: 0.123 0.963, valid loss/acc: 0.204 0.942\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "lr, n_epochs, batch_size = 0.08, 100, 32\n",
        "model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpPsAlXU0T_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d809f073-5eec-4a09-ff67-4164b621f079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test loss = 0.152, acc = 0.958\n"
          ]
        }
      ],
      "source": [
        "# evalute the model on test data\n",
        "\"\"\"\n",
        "hidden layer의 개수를 100, learning rate 0.08, epoch 100, batch size 32로 했을 때 validation data accuracy가 가장 좋아서 이를 통해 test dataset에서 성능을 평가하였다.\n",
        "그 결과 test loss는 0.152, test accuracy는 0.958이 나왔다.\n",
        "\"\"\"\n",
        "Y_hat, _ = model.forward(X_test)\n",
        "test_loss = model.compute_loss(Y_test, Y_hat)\n",
        "test_acc = model.evaluate(Y_test, Y_hat)\n",
        "print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}